{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clona el repositorio de YOLOv5\n",
    "# !git clone https://github.com/ultralytics/yolov5.git\n",
    "# %cd yolov5\n",
    "\n",
    "# # Instala las dependencias\n",
    "# !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load YOLO Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mirvi/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-22 Python-3.9.7 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in C:\\Users\\mirvi/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-22 Python-3.9.7 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.models.yolo.model.YOLO'>\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo YOLOv5\n",
    "model1 = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model2 = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "model3 = YOLO('C:/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte2_psiv2/object_tracking_yolo/yolov8n.pt')\n",
    "\n",
    "print(type(model3))\n",
    "\n",
    "# Definir clases de interés\n",
    "CLASSES_OF_INTEREST = ['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Tracker Class (per centroides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class Tracker:\n",
    "    MAX_DISAPPEAR_LIMIT = 12\n",
    "    def __init__(self,res_p=1):\n",
    "        self.next_unique_id = 0\n",
    "        self.trackers = {}\n",
    "        self.disappear_trackers = {}\n",
    "        self.tracked_bboxes = {}\n",
    "        self.MAX_DISTANCE_THRESHOLD = 100*res_p\n",
    "    \n",
    "    \n",
    "    def init_object(self,centroid,boxes):\n",
    "        global next_unique_id\n",
    "        self.trackers[self.next_unique_id] = centroid\n",
    "        self.tracked_bboxes[self.next_unique_id] = boxes\n",
    "        self.disappear_trackers[self.next_unique_id] = 0\n",
    "        self.next_unique_id+=1\n",
    "\n",
    "    def del_object(self,track_id):\n",
    "        del self.trackers[track_id]\n",
    "        del self.tracked_bboxes[track_id]\n",
    "        del self.disappear_trackers[track_id]\n",
    "\n",
    "    def update_object(self,bboxes):\n",
    "        \n",
    "        # Si no hay detecciones\n",
    "        if(len(bboxes)==0):\n",
    "            \n",
    "            # Incrementar contador de desaparición\n",
    "            for oid in list(self.disappear_trackers.keys()):\n",
    "                self.disappear_trackers[oid]+=1\n",
    "                \n",
    "                # Eliminar objeto si ha desaparecido por más de X frames\n",
    "                if self.disappear_trackers[oid] > Tracker.MAX_DISAPPEAR_LIMIT:\n",
    "                    self.del_object(oid)\n",
    "                \n",
    "            return self.tracked_bboxes\n",
    "        \n",
    "        # Si hay detecciones\n",
    "        else:   \n",
    "\n",
    "            # Calcular centroides de las detecciones\n",
    "            input_centroids = np.zeros((len(bboxes),2)) \n",
    "            for i in range(len(bboxes)):\n",
    "                x,y,w,h = bboxes[i][0],bboxes[i][1],bboxes[i][2],bboxes[i][3]\n",
    "                cx,cy = x + w/2 , y + h/2\n",
    "                input_centroids[i] = (cx,cy)\n",
    "\n",
    "            # Si aun no hay trackers, inicializarlos con las detecciones\n",
    "            if(len(self.trackers)==0):\n",
    "                for i in range(len(input_centroids)):\n",
    "                    self.init_object(input_centroids[i],bboxes[i])\n",
    "            \n",
    "            # Si ya hay trackers, asociar las detecciones a los trackers\n",
    "            else:\n",
    "                # obtener centroides de los trackers\n",
    "                tracker_centroids = list(self.trackers.values())\n",
    "\n",
    "                # Calcular matriz de distancias\n",
    "                distance_matrix = cdist(np.array(tracker_centroids) , input_centroids)\n",
    "\n",
    "                # Ordenar las distancias de menor a mayor\n",
    "                rows = distance_matrix.min(axis=1).argsort()\n",
    "                cols = distance_matrix.argmin(axis=1)[rows]\n",
    "\n",
    "                usedRows = set()\n",
    "                usedCols = set()\n",
    "                \n",
    "                # Asociar ids trackers a detecciones\n",
    "                tracker_ids = list(self.trackers.keys()) \n",
    "                for row,col in zip(rows,cols):\n",
    "                    if row in usedRows or col in usedCols:\n",
    "                        continue\n",
    "\n",
    "                    # Verifica si la distancia es menor que la distancia mínima\n",
    "                    if np.linalg.norm(tracker_centroids[row] - input_centroids[col]) > self.MAX_DISTANCE_THRESHOLD:\n",
    "                        continue  # Skip association if distance is greater than th pixels\n",
    "                    \n",
    "                    # Asociamos el id y bounding boxes del input con los de tracked\n",
    "                    track_id = tracker_ids[row]\n",
    "                    \n",
    "                    self.trackers[track_id] = input_centroids[col]\n",
    "                    self.tracked_bboxes[track_id] = bboxes[col]\n",
    "\n",
    "                    self.disappear_trackers[track_id] = 0\n",
    "                    usedRows.add(row)                                \n",
    "                    usedCols.add(col)\n",
    "\n",
    "                # Calcular filas y columnas no usadas\n",
    "                unusedRows = set(range(0,distance_matrix.shape[0])).difference(usedRows)\n",
    "                unusedCols = set(range(0,distance_matrix.shape[1])).difference(usedCols)\n",
    "\n",
    "                # Si hay más trackers que detecciones\n",
    "                if(distance_matrix.shape[0]>=distance_matrix.shape[1]):\n",
    "                    \n",
    "                    # Incrementar contador de desaparición\n",
    "                    for r in unusedRows: \n",
    "                        track_id = tracker_ids[r]\n",
    "                        self.disappear_trackers[track_id]+=1\n",
    "\n",
    "                        # Eliminar objeto si ha desaparecido por más de X frames\n",
    "                        if(self.disappear_trackers[track_id] > Tracker.MAX_DISAPPEAR_LIMIT):\n",
    "                            self.del_object(track_id)\n",
    "\n",
    "                # Si hay más detecciones que trackers\n",
    "                else:\n",
    "\n",
    "                    # Inicializar nuevos trackers\n",
    "                    for c in unusedCols:                    \n",
    "                        self.init_object(input_centroids[c],bboxes[c])\n",
    "\n",
    "        return self.tracked_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize percent\n",
    "resize_percent = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(\"/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte2_psiv2/data_r2/short_uab_flow.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection and Tracking over video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# PROCESS VIDEO IN GRAYSCALE\n",
    "gray_video_og = []\n",
    "\n",
    "i=0\n",
    "# Configura para comenzar desde el frame 300\n",
    "start_frame = 5000\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "while True:\n",
    "    ret, fr = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # resize frame\n",
    "    fr = cv2.resize(fr, (0, 0), fx=resize_percent, fy=resize_percent)\n",
    "\n",
    "    # # to gray\n",
    "    # fr2 = cv2.cvtColor(fr1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # # apply clahe contrast\n",
    "    # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    # fr = clahe.apply(fr2)\n",
    "\n",
    "    # agafar nomes frames que volem\n",
    "    if i>0:\n",
    "        gray_video_og.append(fr)\n",
    "    if i == 8000:\n",
    "        break\n",
    "    i+=1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 3 cars, 542.5ms\n",
      "Speed: 8.0ms preprocess, 542.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 0\n",
      "\n",
      "0: 640x384 3 cars, 232.4ms\n",
      "Speed: 19.9ms preprocess, 232.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 7\n",
      "\n",
      "0: 640x384 3 cars, 170.5ms\n",
      "Speed: 6.0ms preprocess, 170.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 14\n",
      "\n",
      "0: 640x384 3 cars, 1 truck, 167.5ms\n",
      "Speed: 5.0ms preprocess, 167.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 21\n",
      "\n",
      "0: 640x384 3 cars, 216.4ms\n",
      "Speed: 4.0ms preprocess, 216.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 28\n",
      "\n",
      "0: 640x384 3 cars, 1 truck, 211.4ms\n",
      "Speed: 5.0ms preprocess, 211.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 35\n",
      "\n",
      "0: 640x384 3 cars, 414.9ms\n",
      "Speed: 4.0ms preprocess, 414.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 42\n",
      "\n",
      "0: 640x384 3 cars, 154.5ms\n",
      "Speed: 6.0ms preprocess, 154.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 49\n",
      "\n",
      "0: 640x384 3 cars, 139.6ms\n",
      "Speed: 7.0ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 56\n",
      "\n",
      "0: 640x384 4 cars, 162.6ms\n",
      "Speed: 4.0ms preprocess, 162.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 63\n",
      "\n",
      "0: 640x384 3 cars, 162.6ms\n",
      "Speed: 4.0ms preprocess, 162.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 70\n",
      "\n",
      "0: 640x384 3 cars, 139.6ms\n",
      "Speed: 4.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 77\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 138.6ms\n",
      "Speed: 4.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 84\n",
      "\n",
      "0: 640x384 3 cars, 1 truck, 146.6ms\n",
      "Speed: 4.0ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 91\n",
      "\n",
      "0: 640x384 3 cars, 143.6ms\n",
      "Speed: 6.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 98\n",
      "\n",
      "0: 640x384 3 cars, 146.6ms\n",
      "Speed: 5.0ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 105\n",
      "\n",
      "0: 640x384 2 cars, 1 truck, 149.6ms\n",
      "Speed: 3.0ms preprocess, 149.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 112\n",
      "\n",
      "0: 640x384 1 person, 2 cars, 1 truck, 143.6ms\n",
      "Speed: 3.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 119\n",
      "\n",
      "0: 640x384 3 cars, 137.6ms\n",
      "Speed: 6.0ms preprocess, 137.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 126\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 143.6ms\n",
      "Speed: 3.0ms preprocess, 143.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 133\n",
      "\n",
      "0: 640x384 3 cars, 1 truck, 141.6ms\n",
      "Speed: 4.0ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 140\n",
      "\n",
      "0: 640x384 1 person, 4 cars, 140.6ms\n",
      "Speed: 6.0ms preprocess, 140.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 147\n",
      "\n",
      "0: 640x384 1 person, 4 cars, 137.6ms\n",
      "Speed: 4.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 154\n",
      "\n",
      "0: 640x384 4 cars, 264.3ms\n",
      "Speed: 4.0ms preprocess, 264.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 161\n",
      "\n",
      "0: 640x384 1 person, 5 cars, 140.6ms\n",
      "Speed: 4.0ms preprocess, 140.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 168\n",
      "\n",
      "0: 640x384 5 cars, 136.6ms\n",
      "Speed: 4.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 175\n",
      "\n",
      "0: 640x384 1 person, 4 cars, 140.6ms\n",
      "Speed: 5.0ms preprocess, 140.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 182\n",
      "\n",
      "0: 640x384 3 cars, 132.6ms\n",
      "Speed: 4.0ms preprocess, 132.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 189\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 127.7ms\n",
      "Speed: 5.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 196\n",
      "\n",
      "0: 640x384 3 cars, 131.6ms\n",
      "Speed: 4.0ms preprocess, 131.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 203\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 210\n",
      "\n",
      "0: 640x384 2 persons, 3 cars, 168.5ms\n",
      "Speed: 7.0ms preprocess, 168.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 217\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 136.6ms\n",
      "Speed: 6.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 224\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 133.6ms\n",
      "Speed: 4.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 231\n",
      "\n",
      "0: 640x384 5 cars, 132.6ms\n",
      "Speed: 3.0ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 238\n",
      "\n",
      "0: 640x384 1 person, 4 cars, 133.7ms\n",
      "Speed: 5.0ms preprocess, 133.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 245\n",
      "\n",
      "0: 640x384 1 person, 4 cars, 138.6ms\n",
      "Speed: 16.0ms preprocess, 138.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 252\n",
      "\n",
      "0: 640x384 3 cars, 142.6ms\n",
      "Speed: 4.0ms preprocess, 142.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 259\n",
      "\n",
      "0: 640x384 3 cars, 136.6ms\n",
      "Speed: 6.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 266\n",
      "\n",
      "0: 640x384 3 cars, 132.6ms\n",
      "Speed: 4.0ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 273\n",
      "\n",
      "0: 640x384 3 cars, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 280\n",
      "\n",
      "0: 640x384 3 cars, 122.7ms\n",
      "Speed: 4.0ms preprocess, 122.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 287\n",
      "\n",
      "0: 640x384 3 cars, 126.7ms\n",
      "Speed: 4.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 294\n",
      "\n",
      "0: 640x384 1 person, 3 cars, 124.7ms\n",
      "Speed: 5.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 301\n",
      "\n",
      "0: 640x384 3 cars, 131.6ms\n",
      "Speed: 3.0ms preprocess, 131.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 308\n",
      "\n",
      "0: 640x384 3 cars, 146.6ms\n",
      "Speed: 5.0ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 315\n",
      "\n",
      "0: 640x384 2 persons, 3 cars, 188.5ms\n",
      "Speed: 76.8ms preprocess, 188.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 322\n",
      "\n",
      "0: 640x384 3 cars, 131.6ms\n",
      "Speed: 5.0ms preprocess, 131.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 329\n",
      "\n",
      "0: 640x384 1 person, 5 cars, 127.6ms\n",
      "Speed: 4.0ms preprocess, 127.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "frame id: 336\n",
      "count up: 0\n",
      "count down: 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "\n",
    "time_start_all = time.time()\n",
    "\n",
    "# Define selected model\n",
    "model = model3\n",
    "\n",
    "# resize percent\n",
    "resize_percent = 0.7\n",
    "\n",
    "# open video\n",
    "cap = cv2.VideoCapture(\"/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte2_psiv2/data_r2/short_uab_flow.mp4\")\n",
    "\n",
    "# Init tracker\n",
    "tracker2 = Tracker(res_p=resize_percent)\n",
    "\n",
    "# Define ROI dimensions\n",
    "\n",
    "roix1 = int(150 * resize_percent)\n",
    "roix2 = int(480 * resize_percent)\n",
    "roiy1 = int(320 * resize_percent)\n",
    "roiy2 = int(900 * resize_percent)\n",
    "\n",
    "# Define reference line in the middle of the ROI\n",
    "line_y_position = ((roiy2 - roiy1) // 2) - int(40 * resize_percent)\n",
    "\n",
    "# Counters for cars moving up and down\n",
    "count_up = 0\n",
    "count_down = 0\n",
    "tmp_count_down = 0\n",
    "tmp_count_up = 0\n",
    "\n",
    "# Dictionary to store previous positions of each car\n",
    "previous_positions = {}\n",
    "\n",
    "# Dictionary to track crossed status\n",
    "crossed_up = {id: False for id in range(200)}\n",
    "crossed_down = {id: False for id in range(200)}\n",
    "parked = {id: False for id in range(200)}\n",
    "\n",
    "current_frame_id = []\n",
    "previous_frame_id = []\n",
    "\n",
    "# Distance threshold for movement in pixels\n",
    "movement_threshold = 50*resize_percent\n",
    "\n",
    "# Initialize last positions dictionary\n",
    "last_positions = {}\n",
    "\n",
    "# init box_id var\n",
    "box_id = 0\n",
    "\n",
    "# right roi margin\n",
    "right_margin = 15*resize_percent\n",
    "bottom_margin = 50*resize_percent\n",
    "\n",
    "# open video\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# Definir el codec y crear el objeto VideoWriter para guardar el video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")  # Codec para .avi (también puedes usar 'MP4V' para .mp4)\n",
    "out = cv2.VideoWriter('short_output', fourcc, 30, (width, height))\n",
    "\n",
    "# variable for loop end\n",
    "fi = True\n",
    "\n",
    "# set video to frame 7000\n",
    "frame_id = 0\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "while fi:\n",
    "    # Start time measurement\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Process the frame\n",
    "    # fr = gray_video_og[frame_id]\n",
    "    # frame = fr.copy()\n",
    "    ret,fr = cap.read()\n",
    "\n",
    "    # si es el final, fem una ultima iteracio per actualitzar els cotxes que queden\n",
    "    if not ret:\n",
    "        fi = False\n",
    "        fr = np.zeros((height, width, 3), np.uint8)\n",
    "    \n",
    "    frame = cv2.resize(fr, (0, 0), fx=resize_percent, fy=resize_percent)\n",
    "\n",
    "    # Extract ROI\n",
    "    roi = frame[roiy1:roiy2, roix1:roix2]\n",
    "\n",
    "    # 1. Object Detection\n",
    "    ts_od = time.time()\n",
    "    results = model(roi)\n",
    "\n",
    "    # Filter detections based on size and movement\n",
    "    detections = []\n",
    "    for idx,det in enumerate(results[0].boxes):\n",
    "        x1, y1, x2, y2, conf, cls = det.data[0]\n",
    "        if cls == 2:\n",
    "            w = int(x2 - x1)\n",
    "            h = int(y2 - y1)\n",
    "            x = int(x1)\n",
    "            y = int(y1)\n",
    "\n",
    "            # Only keep detections with a certain size\n",
    "            if w > 50 * resize_percent and h > 50 * resize_percent:\n",
    "                detections.append([x, y, w, h])\n",
    "\n",
    "    te_od = time.time()\n",
    "\n",
    "    # Draw text and ROI with reference line\n",
    "    cv2.putText(frame, f'ObjDet time: {(te_od - ts_od):.2f}', \n",
    "                (int(10 * resize_percent), int(30 * resize_percent)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"UP: {count_up}\", (int(10 * resize_percent), int(700 * resize_percent)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f\"DOWN: {count_down}\", (int(10 * resize_percent), int(750 * resize_percent)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.rectangle(frame, (roix1, roiy1), (roix2, roiy2), (0, 255, 125), 2)\n",
    "\n",
    "    # Draw line with increments\n",
    "    line_inc = [-80, -10, 20]\n",
    "    for inc in line_inc:\n",
    "        cv2.line(roi, (0, line_y_position + inc), (width, line_y_position + inc), (0, 255, 255), 2)\n",
    "\n",
    "    # 2. Object Tracking\n",
    "    boxes_ids = tracker2.update_object(detections)\n",
    "\n",
    "    # Draw bounding boxes and text for tracked objects\n",
    "    for box_id, box in boxes_ids.items():\n",
    "        x, y, w, h = box\n",
    "        id = box_id\n",
    "\n",
    "        # add to current frame id\n",
    "        current_frame_id.append(id)\n",
    "\n",
    "        # check if car is parked\n",
    "        if x + w + roix1 > roix2 - right_margin:\n",
    "            parked[id] = True\n",
    "\n",
    "        # Draw ID and bounding box\n",
    "        if parked[id]:\n",
    "            cv2.putText(roi, f\"PARKED {str(id)}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 127, 0), 1)\n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (200, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(roi, str(id), (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 127, 0), 1)\n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Check if the car crossed the line\n",
    "        current_y = y + h // 2\n",
    "        if id not in previous_positions:\n",
    "            previous_positions[id] = current_y\n",
    "            continue\n",
    "\n",
    "        previous_y = previous_positions[id]\n",
    "\n",
    "        # Check if parked is no more parked and going down\n",
    "        if parked[id] and x + w + roix1 < roix2 - right_margin:\n",
    "                # chek each line\n",
    "                for inc in line_inc:\n",
    "                    # Check crossing of reference line down\n",
    "                    if previous_y < line_y_position + int(inc * resize_percent) and current_y > line_y_position + int(inc * resize_percent) and not crossed_down[id]:\n",
    "                        tmp_count_down += 1\n",
    "                        crossed_down[id] = True\n",
    "                        parked[id] = False\n",
    "                        break\n",
    "\n",
    "        # Check if non-parked car crossed the line\n",
    "        elif not crossed_up[id] and not crossed_down[id] and not parked[id]:\n",
    "\n",
    "            for inc in line_inc:\n",
    "                # Check crossing of reference line\n",
    "                if previous_y < line_y_position + int(inc * resize_percent) and current_y > line_y_position + int(inc * resize_percent) and not crossed_down[id]:\n",
    "                    tmp_count_down += 1\n",
    "                    crossed_down[id] = True\n",
    "                    break\n",
    "                elif previous_y > line_y_position + int(inc * resize_percent) and current_y < line_y_position + int(inc * resize_percent) and not crossed_up[id]:\n",
    "                    tmp_count_up += 1\n",
    "                    crossed_up[id] = True\n",
    "                    break\n",
    "\n",
    "        # Update the car's previous position\n",
    "        previous_positions[id] = current_y\n",
    "\n",
    "\n",
    "\n",
    "    # if last frame set current cars to 0\n",
    "    if not fi:\n",
    "        current_frame_id = []\n",
    "\n",
    "    # Check if any car has left at last frame\n",
    "    for id in previous_frame_id:\n",
    "        if id not in current_frame_id:\n",
    "            \n",
    "            # update counters if car not parked or if car parked but came from bottom or if car came of bottom corner\n",
    "            if not parked[id] or (parked[id] and crossed_up[id])  or (parked[id] and previous_positions[id] + roiy1 > roiy2 - 80):\n",
    "                if crossed_down[id]:\n",
    "                    count_down += 1\n",
    "                elif crossed_up[id]:\n",
    "                    count_up += 1\n",
    "        \n",
    "\n",
    "    # Update previous frame id\n",
    "    previous_frame_id = current_frame_id.copy()\n",
    "    current_frame_id = []\n",
    "\n",
    "    # Calculate FPS\n",
    "    time_end = time.time()\n",
    "    time_per_frame = time_end - time_start\n",
    "    fps = 1 / time_per_frame if time_per_frame > 0 else 0\n",
    "    cv2.putText(frame, f'Processing time: {fps:.2f} FPS', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    # Save output\n",
    "    out.write(frame)\n",
    "\n",
    "    # Show frames\n",
    "    cv2.imshow(\"roi\", roi)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    print('frame id:', frame_id)\n",
    "\n",
    "    # Salta al siguiente frame 7 posiciones adelante\n",
    "    frame_id += 7\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id + 7)\n",
    "\n",
    "    # Exit with ESC key\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "\n",
    "time_end_all = time.time()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()\n",
    "\n",
    "print('count up:', count_up)\n",
    "print('count down:', count_down)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCUL TEMPS FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 8.92 seconds\n",
      "Total frames: 308\n",
      "FPS: 34.55\n"
     ]
    }
   ],
   "source": [
    "final_time = time_end_all - time_start_all\n",
    "total_frames = 308\n",
    "\n",
    "print(f\"Total time: {final_time:.2f} seconds\")\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"FPS: {total_frames / final_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'n'\n",
    "if not p:\n",
    "    print('si')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
